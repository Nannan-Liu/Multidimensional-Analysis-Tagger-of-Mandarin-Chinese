{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['女士们', '，', '先生们', '，', '上午好', '。', ...]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk \n",
    "from nltk.corpus import CategorizedPlaintextCorpusReader\n",
    "corpus = CategorizedPlaintextCorpusReader(\n",
    "    '/users/nannanliu/Python/SCIPPC/original_Chinese/length_controlled/raw',\n",
    "    r'(?!\\.).*\\.txt',\n",
    "    cat_pattern=os.path.join(r'(neg|pos)', '.*',),\n",
    "    encoding='utf-8')\n",
    "corpus.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "df = pd.read_csv(\"/users/nannanliu/Research/MD/stats/individual/OC.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>OC</th>\n",
       "      <th>AMP</th>\n",
       "      <th>AWL</th>\n",
       "      <th>ASL</th>\n",
       "      <th>ASL_std</th>\n",
       "      <th>ACL</th>\n",
       "      <th>ACL_std</th>\n",
       "      <th>CAUS</th>\n",
       "      <th>CONC</th>\n",
       "      <th>...</th>\n",
       "      <th>classical_syntax</th>\n",
       "      <th>disyllabic_words</th>\n",
       "      <th>HSK_1</th>\n",
       "      <th>HSK_3</th>\n",
       "      <th>HSK_6</th>\n",
       "      <th>honourifics</th>\n",
       "      <th>humbles</th>\n",
       "      <th>abstract</th>\n",
       "      <th>unique</th>\n",
       "      <th>english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>OC_2003_01</td>\n",
       "      <td>15.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>17.31</td>\n",
       "      <td>11.71</td>\n",
       "      <td>6.96</td>\n",
       "      <td>4.35</td>\n",
       "      <td>0.78</td>\n",
       "      <td>2.73</td>\n",
       "      <td>...</td>\n",
       "      <td>2.73</td>\n",
       "      <td>60.04</td>\n",
       "      <td>259.65</td>\n",
       "      <td>123.20</td>\n",
       "      <td>267.84</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.90</td>\n",
       "      <td>189.08</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>OC_2003_02</td>\n",
       "      <td>13.25</td>\n",
       "      <td>1.58</td>\n",
       "      <td>14.25</td>\n",
       "      <td>10.62</td>\n",
       "      <td>6.66</td>\n",
       "      <td>4.89</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.99</td>\n",
       "      <td>...</td>\n",
       "      <td>5.30</td>\n",
       "      <td>48.68</td>\n",
       "      <td>239.74</td>\n",
       "      <td>141.06</td>\n",
       "      <td>239.07</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.06</td>\n",
       "      <td>167.55</td>\n",
       "      <td>1.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>OC_2004_01</td>\n",
       "      <td>14.55</td>\n",
       "      <td>1.54</td>\n",
       "      <td>20.75</td>\n",
       "      <td>15.78</td>\n",
       "      <td>6.79</td>\n",
       "      <td>4.43</td>\n",
       "      <td>2.68</td>\n",
       "      <td>1.15</td>\n",
       "      <td>...</td>\n",
       "      <td>3.83</td>\n",
       "      <td>42.13</td>\n",
       "      <td>260.44</td>\n",
       "      <td>159.33</td>\n",
       "      <td>222.14</td>\n",
       "      <td>5.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.62</td>\n",
       "      <td>184.99</td>\n",
       "      <td>1.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>OC_2004_02</td>\n",
       "      <td>11.68</td>\n",
       "      <td>1.53</td>\n",
       "      <td>19.88</td>\n",
       "      <td>17.07</td>\n",
       "      <td>7.29</td>\n",
       "      <td>4.80</td>\n",
       "      <td>3.39</td>\n",
       "      <td>3.39</td>\n",
       "      <td>...</td>\n",
       "      <td>4.90</td>\n",
       "      <td>39.95</td>\n",
       "      <td>249.91</td>\n",
       "      <td>162.83</td>\n",
       "      <td>214.10</td>\n",
       "      <td>1.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>77.27</td>\n",
       "      <td>186.20</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>OC_2005_01</td>\n",
       "      <td>15.61</td>\n",
       "      <td>1.54</td>\n",
       "      <td>18.06</td>\n",
       "      <td>12.10</td>\n",
       "      <td>6.83</td>\n",
       "      <td>4.37</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>...</td>\n",
       "      <td>5.20</td>\n",
       "      <td>57.65</td>\n",
       "      <td>258.21</td>\n",
       "      <td>143.31</td>\n",
       "      <td>236.99</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.86</td>\n",
       "      <td>184.55</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0          OC    AMP   AWL    ASL  ASL_std   ACL  ACL_std  CAUS  \\\n",
       "0           0  OC_2003_01  15.59  1.56  17.31    11.71  6.96     4.35  0.78   \n",
       "1           1  OC_2003_02  13.25  1.58  14.25    10.62  6.66     4.89  0.66   \n",
       "2           2  OC_2004_01  14.55  1.54  20.75    15.78  6.79     4.43  2.68   \n",
       "3           3  OC_2004_02  11.68  1.53  19.88    17.07  7.29     4.80  3.39   \n",
       "4           4  OC_2005_01  15.61  1.54  18.06    12.10  6.83     4.37  2.00   \n",
       "\n",
       "   CONC   ...     classical_syntax  disyllabic_words   HSK_1   HSK_3   HSK_6  \\\n",
       "0  2.73   ...                 2.73             60.04  259.65  123.20  267.84   \n",
       "1  0.99   ...                 5.30             48.68  239.74  141.06  239.07   \n",
       "2  1.15   ...                 3.83             42.13  260.44  159.33  222.14   \n",
       "3  3.39   ...                 4.90             39.95  249.91  162.83  214.10   \n",
       "4  0.80   ...                 5.20             57.65  258.21  143.31  236.99   \n",
       "\n",
       "   honourifics  humbles  abstract  unique  english  \n",
       "0         1.95      0.0     72.90  189.08     0.00  \n",
       "1         0.66      0.0     91.06  167.55     1.32  \n",
       "2         5.74      0.0     89.62  184.99     1.15  \n",
       "3         1.13      0.0     77.27  186.20     0.00  \n",
       "4         4.00      0.0     74.86  184.55     0.40  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynlpir\n",
    "pynlpir.open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OC_2003_01_SEG.txt\n",
      "OC_2003_02_SEG.txt\n",
      "OC_2004_01_SEG.txt\n",
      "OC_2004_02_SEG.txt\n",
      "OC_2005_01_SEG.txt\n",
      "OC_2005_02_SEG.txt\n",
      "OC_2006_01_SEG.txt\n",
      "OC_2006_02_SEG.txt\n",
      "OC_2007_01_SEG.txt\n",
      "OC_2007_02_SEG.txt\n",
      "OC_2013_01_SEG.txt\n",
      "OC_2013_02_SEG.txt\n",
      "OC_2014_01_SEG.txt\n",
      "OC_2014_02_SEG.txt\n",
      "OC_2015_01_SEG.txt\n",
      "OC_2015_02_SEG.txt\n",
      "OC_2015_03_SEG.txt\n",
      "OC_2016_01_SEG.txt\n",
      "OC_2016_02_SEG.txt\n",
      "OC_2016_03_SEG.txt\n",
      "OC_2017_01_SEG.txt\n",
      "OC_2017_02_SEG.txt\n",
      "OC_2017_03_SEG.txt\n"
     ]
    }
   ],
   "source": [
    "#ToRCH contains 15 text types, here get type A OCage\n",
    "import fileinput\n",
    "import fnmatch\n",
    "\n",
    "files=corpus.fileids()\n",
    "OC_files=[]\n",
    "\n",
    "for f in files: \n",
    "    if fnmatch.fnmatch(f, 'OC_20*.txt'):\n",
    "        print (f)\n",
    "        OC_files.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora=[]\n",
    "for file in OC_files: \n",
    "    sub_corpora=corpus.raw(file)\n",
    "    corpora.append(sub_corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "part of speech not recognized: 'proper-noun'\n",
      "part of speech not recognized: 'proper-noun'\n",
      "part of speech not recognized: 'proper-noun'\n",
      "part of speech not recognized: 'proper-noun'\n",
      "part of speech not recognized: 'n_new'\n",
      "part of speech not recognized: 'proper-noun'\n",
      "part of speech not recognized: 'n_new'\n",
      "part of speech not recognized: 'n_new'\n",
      "part of speech not recognized: 'proper-noun'\n",
      "part of speech not recognized: 'proper-noun'\n",
      "part of speech not recognized: 'n_new'\n",
      "part of speech not recognized: 'proper-noun'\n",
      "part of speech not recognized: 'proper-noun'\n",
      "part of speech not recognized: 'proper-noun'\n",
      "part of speech not recognized: 'proper-noun'\n",
      "part of speech not recognized: 'n_new'\n",
      "part of speech not recognized: 'proper-noun'\n"
     ]
    }
   ],
   "source": [
    "tagged_files=[]\n",
    "for sub_corpora in corpora: \n",
    "    tagged_file=pynlpir.segment(sub_corpora, pos_tagging=True, pos_names='child')\n",
    "    tagged_files.append(tagged_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_list=[]\n",
    "for file in tagged_files: \n",
    "    none_list.append([s for s in file if None in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "print (none_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(23): \n",
    "    for n, i in enumerate(tagged_files[j]):\n",
    "        if i == ('\\r新华社', None):\n",
    "            tagged_files[j][n] = ('\\r新华社', 'noun-proper')\n",
    "        if i == ('新华社', None):\n",
    "            tagged_files[j][n] = ('新华社', 'noun-proper')\n",
    "        if i == ('\\r新华网', None):\n",
    "            tagged_files[j][n] = ('\\r新华网', 'noun-proper')\n",
    "        if i == ('新华网', None):\n",
    "            tagged_files[j][n] = ('新华网', 'noun-proper')\n",
    "        if i == ('中新网', None):\n",
    "            tagged_files[j][n] = ('中新网', 'noun-proper')\n",
    "        if i == ('人民网', None):\n",
    "            tagged_files[j][n] = ('人民网', 'noun-proper')\n",
    "        if i == ('\\r中国青年网', None):\n",
    "            tagged_files[j][n] = ('\\r中国青年网', 'noun-proper')\n",
    "        if i == ('中评社', None):\n",
    "            tagged_files[j][n] = ('中评社', 'noun-proper')\n",
    "        if i == ('\\r中国日报网', None):\n",
    "            tagged_files[j][n] = ('\\r中国日报网', 'noun-proper')\n",
    "        if i == ('南华早报', None):\n",
    "            tagged_files[j][n] = ('南华早报', 'noun-proper')\n",
    "        if i == ('\\r国际在线', None):\n",
    "            tagged_files[j][n] = ('\\r国际在线', 'noun-proper')\n",
    "        if i == ('新华社', None):\n",
    "            tagged_files[j][n] = ('新华社', 'noun-proper')\n",
    "        if i == ('派', None): \n",
    "            tagged_files[j][n] = ('派', 'noun-verb')\n",
    "        if i == ('网民', None): \n",
    "            tagged_files[j][n] = ('网民', 'noun')\n",
    "        if i == ('屌丝', None):\n",
    "            tagged_files[j][n] = ('屌丝', 'noun')\n",
    "        if i == ('\\r屌丝', None):\n",
    "            tagged_files[j][n] = ('\\r屌丝', 'noun')\n",
    "        if i == ('富帅', None):\n",
    "            tagged_files[j][n] = ('富帅', 'noun')\n",
    "        if i == ('解构', None): \n",
    "            tagged_files[j][n] = ('解构', 'noun-verb')\n",
    "        if i == ('身份卑微', None): \n",
    "            tagged_files[j][n] = ('身份卑微', 'adjective')\n",
    "        if i == ('\\r南方日报', None): \n",
    "            tagged_files[j][n] = ('\\r南方日报', 'noun')\n",
    "        if i == ('法新社', None):\n",
    "            tagged_files[j][n] = ('法新社', 'noun-proper')\n",
    "        if i == ('美联社', None):\n",
    "            tagged_files[j][n] = ('美联社', 'noun-proper')\n",
    "        if i == ('路透社', None):\n",
    "            tagged_files[j][n] = ('路透社', 'noun-proper')\n",
    "        if i == ('环球时报', None):\n",
    "            tagged_files[j][n] = ('环球时报', 'noun-proper')\n",
    "        if i == ('飞机', None):\n",
    "            tagged_files[j][n] = ('飞机', 'noun')\n",
    "        if i == ('甲', None): \n",
    "            tagged_files[j][n] = ('甲', 'numeral')\n",
    "        if i == ('乙', None): \n",
    "            tagged_files[j][n] = ('乙', 'numeral')\n",
    "        if i == ('丙', None): \n",
    "            tagged_files[j][n] = ('丙', 'numeral')\n",
    "        if i == ('丁', None): \n",
    "            tagged_files[j][n] = ('丁', 'numeral')\n",
    "        if i == ('辰', None): \n",
    "            tagged_files[j][n] = ('辰', 'numeral')\n",
    "        if i == ('癸', None): \n",
    "            tagged_files[j][n] = ('癸', 'numeral')  \n",
    "        if i == ('戊', None): \n",
    "            tagged_files[j][n] = ('戊', 'numeral')\n",
    "        if i == ('巳', None): \n",
    "            tagged_files[j][n] = ('巳', 'numeral')\n",
    "        if i == ('\\u3000', None): \n",
    "            tagged_files[j][n] = ('\\u3000', 'None')\n",
    "        if i == ('贴吧', None): \n",
    "            tagged_files[j][n] = ('贴吧', 'noun')\n",
    "        if i == (' ', None): \n",
    "            tagged_files[j][n] = (' ', 'empty')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "print (none_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 5 by passives\n",
    "bypa=['被', '受', '受到', '遭受', '挨', '加以']\n",
    "def bypa(text_type): \n",
    "    def raw(text_type): \n",
    "        return text_type.count(('被', 'preposition 被'))+text_type.count(('受到', 'preposition 受到'))\\\n",
    "    +text_type.count(('遭受', 'verb'))+text_type.count(('受', 'verb'))+text_type.count(('遭', 'verb'))+text_type.count(('挨', 'verb'))\\\n",
    "    +text_type.count(('加以', 'performative verb'))\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bypa_result=[]\n",
    "for file in tagged_files: \n",
    "    bypa_result.append(bypa(file))\n",
    "    \n",
    "df['BYPA'] = pd.Series(bypa_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "   # print (bypa(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 8 COND 条件连词、副词\n",
    "#如果……（那么）、只有……（才）、假如、除非、要是、要不是、只要、假使、假如、倘若、倘或、倘、设使、设若、如若、若\n",
    "def cond(text_type):\n",
    "    def raw(text_type): \n",
    "        return str(text_type).count('如果')+str(text_type).count('只有')\\\n",
    "    +str(text_type).count('假如')+str(text_type).count('除非')\\\n",
    "    +str(text_type).count('要是')+str(text_type).count('要不是')\\\n",
    "    +str(text_type).count('只要')+str(text_type).count('假如')\\\n",
    "    +str(text_type).count('倘若')+str(text_type).count('倘或')\\\n",
    "    +str(text_type).count('设使')+str(text_type).count('设若')\\\n",
    "    +str(text_type).count('如若')+str(text_type).count('若')\\\n",
    "    +text_type.count(('的话', 'particle 的话'))+\\\n",
    "    str(text_type).count(\"('的', 'particle 的/底'), ('时候', 'noun')\")\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_result=[]\n",
    "for file in tagged_files: \n",
    "    cond_result.append(cond(file))\n",
    "    \n",
    "df['COND'] = pd.Series(cond_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 14 other conjunctions\n",
    "def other_conj(text_type):\n",
    "    def raw(text_type):\n",
    "        return str(text_type).count('conjunction')-str(text_type).count('coordinating conjunction')-text_type.count(('因为', 'conjunction'))-text_type.count(('固', 'conjunction'))-text_type.count(('所以', 'conjunction'))-text_type.count(('则', 'conjunction'))-text_type.count(('从而', 'conjunction'))-text_type.count(('故', 'conjunction'))-text_type.count(('结果', 'conjunction'))-text_type.count(('所以', 'conjunction'))-text_type.count(('为此', 'conjunction'))-text_type.count(('以至', 'conjunction'))-text_type.count(('以至于', 'conjunction'))-text_type.count(('因', 'conjunction'))-text_type.count(('因此', 'conjunction'))-text_type.count(('因而', 'conjunction'))-text_type.count(('由于', 'conjunction'))-text_type.count(('于是', 'conjunction'))-text_type.count(('之所以', 'conjunction'))-text_type.count(('致使', 'conjunction'))-text_type.count(('纵然', 'conjunction'))-text_type.count(('即使', 'conjunction'))-text_type.count(('虽然', 'conjunction'))-text_type.count(('虽说', 'conjunction'))-text_type.count(('固然', 'conjunction'))-text_type.count(('尽管', 'conjunction'))-text_type.count(('就是', 'conjunction'))-text_type.count(('纵然', 'conjunction'))-text_type.count(('即使', 'conjunction'))-text_type.count(('虽然', 'conjunction'))-text_type.count(('虽说', 'conjunction'))-text_type.count(('尽管', 'conjunction'))-text_type.count(('就是', 'conjunction'))-text_type.count(('如果', 'conjunction'))-text_type.count(('只有', 'conjunction'))-text_type.count(('假如', 'conjunction'))-text_type.count(('除非', 'conjunction'))-text_type.count(('要是', 'conjunction'))-text_type.count(('要不是', 'conjunction'))-text_type.count(('只要', 'conjunction'))-text_type.count(('假如', 'conjunction'))-text_type.count(('倘若', 'conjunction'))-text_type.count(('倘或', 'conjunction'))-text_type.count(('设使', 'conjunction'))-text_type.count(('设若', 'conjunction'))-text_type.count(('如若', 'conjunction'))-text_type.count(('若', 'conjunction'))\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_conj_result=[]\n",
    "for file in tagged_files: \n",
    "    other_conj_result.append(other_conj(file))\n",
    "    \n",
    "df['other_conj'] = pd.Series(other_conj_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 14 modifying adverbs\n",
    "def modify_adv(text_type): \n",
    "    def raw(text_type): \n",
    "        return text_type.count(('也', 'adverb'))+\\\n",
    "    text_type.count(('都', 'adverb'))+text_type.count(('又', 'adverb'))\\\n",
    "    +text_type.count(('才', 'adverb'))+text_type.count(('就', 'adverb'))\\\n",
    "    +text_type.count(('就是', 'adverb'))+text_type.count(('倒是', 'adverb'))\\\n",
    "    +text_type.count(('越来越', 'adverb'))+text_type.count(('一边', 'adverb'))\\\n",
    "    +text_type.count(('再', 'adverb'))+text_type.count(('甚至', 'adverb'))\\\n",
    "    +text_type.count(('连', 'particle 连'))+text_type.count(('却', 'adverb'))\\\n",
    "    +text_type.count(('原本', 'adverb'))+text_type.count(('只', 'adverb'))\\\n",
    "    +text_type.count(('毕竟', 'adverb'))+text_type.count(('仍然', 'adverb'))\\\n",
    "    +text_type.count(('反正', 'adverb'))+text_type.count(('等', 'particle 等/等等/云云'))\\\n",
    "    +text_type.count(('刚', 'adverb'))+text_type.count(('常常', 'adverb'))\\\n",
    "    +text_type.count(('已经', 'adverb'))+text_type.count(('就要', 'adverb'))\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "modify_adv_result=[]\n",
    "for file in tagged_files: \n",
    "    modify_adv_result.append(modify_adv(file))\n",
    "    \n",
    "df['modify_adv'] = pd.Series(modify_adv_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 11 demonstrative pronoun\n",
    "def demp(text_type): \n",
    "    def raw(text_type): \n",
    "        return str(text_type).count('demonstrative pronoun')\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "demp_result=[]\n",
    "for file in tagged_files: \n",
    "    demp_result.append(demp(file))\n",
    "    \n",
    "df['DEMP'] = pd.Series(demp_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (demp(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 7 Be是\n",
    "def be(text_type):\n",
    "    def raw(text_type): \n",
    "        return text_type.count(('是', 'verb 是'))\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "be_result=[]\n",
    "for file in tagged_files: \n",
    "    be_result.append(be(file))\n",
    "    \n",
    "df['BE'] = pd.Series(be_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for corpus in sub_corpora: \n",
    "    #print (be(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 15 EX有\n",
    "def ex(text_type): \n",
    "    def raw(text_type): \n",
    "        return str(text_type).count('verb 有')\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_result=[]\n",
    "for file in tagged_files: \n",
    "    ex_result.append(ex(file))\n",
    "    \n",
    "df['EX'] = pd.Series(ex_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.92\n",
      "12.26\n",
      "6.89\n",
      "6.03\n",
      "8.81\n",
      "7.84\n",
      "3.15\n",
      "8.3\n",
      "5.03\n",
      "5.46\n",
      "6.5\n",
      "7.79\n",
      "10.01\n",
      "9.64\n",
      "13.54\n",
      "7.64\n",
      "8.08\n",
      "10.41\n",
      "9.1\n",
      "8.6\n",
      "11.59\n",
      "8.88\n",
      "12.12\n"
     ]
    }
   ],
   "source": [
    "for file in tagged_files: \n",
    "    print (ex(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 18 other personal pronouns apart from FPP, SPP, TPP\n",
    "def other_personal(text_type): \n",
    "    def raw(text_type): \n",
    "        return str(text_type).count('personal pronoun')-str(text_type).count('我')\\\n",
    "    -str(text_type).count('你')-str(text_type).count('她')-str(text_type).count('他')\\\n",
    "    -str(text_type).count('它')\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_personal_result=[]\n",
    "for file in tagged_files: \n",
    "    other_personal_result.append(other_personal(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['other_personal'] = pd.Series(other_personal_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (other_personal(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 18 other personal pronouns apart from FPP, SPP, TPP\n",
    "def interrogative(text_type): \n",
    "    def raw(text_type): \n",
    "        return str(text_type).count('interrogative pronoun')-\\\n",
    "    str(text_type).count('predicative interrogative pronoun')\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "interrogative_result=[]\n",
    "for file in tagged_files: \n",
    "    interrogative_result.append(interrogative(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['interrogative'] = pd.Series(interrogative_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (interrogative(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 19 all adjectives \n",
    "def jj(text_type): \n",
    "    def raw(text_type): \n",
    "        return str(text_type).count('adjective')-str(text_type).count('noun-adjective')-\\\n",
    "    str(text_type).count('auxiliary adjective')\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "jj_result=[]\n",
    "for file in tagged_files: \n",
    "    jj_result.append(jj(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['JJ'] = pd.Series(jj_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (jj(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 23 noun nouns, all other nouns\n",
    "#noun - noun-adjective - noun-verb - pronoun-\n",
    "#personal pronoun - predicate demonstrative pronoun \n",
    "#-demonstrative pronoun - locative demonstrative pronoun \n",
    "#- predicate interrogative pronoun\n",
    "#- interrogative pronoun (什么) - predicate demonstrative pronoun\n",
    "def noun(text_type):\n",
    "    def raw(text_type): \n",
    "        return str(text_type).count('noun')-str(text_type).count('noun-adjective')\\\n",
    "    -str(text_type).count('noun-verb')-str(text_type).count('pronoun')-\\\n",
    "    str(text_type).count('noun of locality')+str(text_type).count('noun morpheme')\\\n",
    "    +str(text_type).count('proper noun')\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_result=[]\n",
    "for file in tagged_files: \n",
    "    noun_result.append(noun(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['noun'] = pd.Series(noun_result)\n",
    "#del df['NN']\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (noun(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 27 noun morpheme\n",
    "#def noun_morpheme(text_type):\n",
    "    #def raw(text_type): \n",
    "       # return str(text_type).count('noun morpheme')\n",
    "    #def normalized(text_type): \n",
    "        #return raw(text_type) / len(text_type)\n",
    "    #return round(normalized (text_type) * 1000, 2)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noun_morpheme_result=[]\n",
    "#for file in tagged_files: \n",
    "    #noun_morpheme_result.append(noun_morpheme(file))\n",
    "    \n",
    "#df['noun_morpheme'] = pd.Series(noun_morpheme_result)\n",
    "#df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (noun_morpheme(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del df['noun_morpheme']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 24 nominalization NOMZ \n",
    "#noun-adjective, noun-verb\n",
    "def nomz(text_type):\n",
    "    def raw(text_type): \n",
    "        return str(text_type).count('noun-adjective')+str(text_type).count('noun-verb')\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "nomz_result=[]\n",
    "for file in tagged_files: \n",
    "    nomz_result.append(nomz(file))\n",
    "    \n",
    "df['NOMZ'] = pd.Series(nomz_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (nomz(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 28 proper noun + organization/group name\n",
    "#def proper_noun(text_type):\n",
    "    #def raw(text_type): \n",
    "       # return str(text_type).count('proper noun')+str(text_type).count('organization/group name')\n",
    "    #def normalized(text_type): \n",
    "        #return raw(text_type) / len(text_type)\n",
    "    #return round(normalized (text_type) * 1000, 2)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proper_noun_result=[]\n",
    "#for file in tagged_files: \n",
    "    #proper_noun_result.append(proper_noun(file))\n",
    "\n",
    "#df['proper_noun'] = pd.Series(proper_noun_result)\n",
    "#df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (proper_noun(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del df['proper_noun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 26 phrasal coordinations PHC \n",
    "# same tags before and after 和、以及、与、并、及、暨\n",
    "#ICTCLAS coordinating conjunction\n",
    "def phc(text_type):\n",
    "    def raw(text_type):\n",
    "        return str(text_type).count('coordinating conjunction')\n",
    "    def normalized(text_type):                                                                                     \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "phc_result=[]\n",
    "for file in tagged_files: \n",
    "    phc_result.append(phc(file))\n",
    "    \n",
    "df['PHC'] = pd.Series(phc_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (phc(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 27 PIN prepositions 介词\n",
    "#all propositions except for bpin_list=['按着', '按照', '本着', '朝着', '趁着', '出于',\\\n",
    "#           '待到', '对于', '根据', '关于', '基于', '鉴于',\\\n",
    "#           '借着', '经过', '靠着', '冒着', '面对', '面临', \\\n",
    "#           '凭借', '顺着', '随着', '通过', '为了', '围绕',\\\n",
    "#           '向着', '沿着', '依据', '针对']\n",
    "def pin(text_type):\n",
    "    def raw(text_type): \n",
    "        return str(text_type).count('preposition')-text_type.count(('按着', 'preposition'))-text_type.count(('按照', 'preposition'))-text_type.count(('本着', 'preposition'))-text_type.count(('朝着', 'preposition'))-text_type.count(('趁着', 'preposition'))-text_type.count(('出于', 'preposition'))-text_type.count(('待到', 'preposition'))-text_type.count(('对于', 'preposition'))-text_type.count(('根据', 'preposition'))-text_type.count(('关于', 'preposition'))-text_type.count(('基于', 'preposition'))-text_type.count(('鉴于', 'preposition'))-text_type.count(('借着', 'preposition'))-text_type.count(('经过', 'preposition'))-text_type.count(('靠着', 'preposition'))-text_type.count(('冒着', 'preposition'))-text_type.count(('面对', 'preposition'))-text_type.count(('面临', 'preposition'))-text_type.count(('凭借', 'preposition'))-text_type.count(('顺着', 'preposition'))-text_type.count(('随着', 'preposition'))-text_type.count(('通过', 'preposition'))-text_type.count(('为了', 'preposition'))-text_type.count(('围绕', 'preposition'))-text_type.count(('向着', 'preposition'))-text_type.count(('沿着', 'preposition'))-text_type.count(('依据', 'preposition'))-text_type.count(('针对', 'preposition')) \n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pin_result=[]\n",
    "for file in tagged_files: \n",
    "    pin_result.append(pin(file))\n",
    "    \n",
    "df['PIN'] = pd.Series(pin_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (pin(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 28 BPIN disyllabic prepositions\n",
    "#按照、本着、按着、朝着、趁着、出于、待到、对于、根据、关于、基于、鉴于\n",
    "#借着、经过、靠着、冒着、面对、面临、凭借、顺着、随着、通过、为了\n",
    "#围绕、向着、沿着、依据、针对\n",
    "def bpin(text_type):\n",
    "    def raw(text_type):\n",
    "        return text_type.count(('按照', 'preposition'))+text_type.count(('本着', 'preposition'))+text_type.count(('按着', 'preposition'))+text_type.count(('朝着', 'preposition'))+text_type.count(('趁着', 'preposition'))+text_type.count(('出于', 'preposition'))+text_type.count(('待到', 'preposition'))+text_type.count(('对于', 'preposition'))+text_type.count(('根据', 'preposition'))+text_type.count(('关于', 'preposition'))+text_type.count(('基于', 'preposition'))+text_type.count(('鉴于', 'preposition'))+text_type.count(('借着', 'preposition'))+text_type.count(('经过', 'preposition'))+text_type.count(('靠着', 'preposition'))+text_type.count(('冒着', 'preposition'))+text_type.count(('面对', 'preposition'))+text_type.count(('面临', 'preposition'))+text_type.count(('凭借', 'preposition'))+text_type.count(('顺着', 'preposition'))+text_type.count(('随着', 'preposition'))+text_type.count(('通过', 'preposition'))+text_type.count(('为了', 'preposition'))+text_type.count(('围绕', 'preposition'))+text_type.count(('向着', 'preposition'))+text_type.count(('沿着', 'preposition'))+text_type.count(('依据', 'preposition'))\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpin_result=[]\n",
    "for file in tagged_files: \n",
    "    bpin_result.append(bpin(file))\n",
    "    \n",
    "df['BPIN'] = pd.Series(bpin_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (bpin(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 29 place\n",
    "def place(text_type):\n",
    "    def raw(text_type): \n",
    "        return str(text_type).count('noun of locality')+str(text_type).count('locative word')+\\\n",
    "    str(text_type).count('toponym')\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_result=[]\n",
    "for file in tagged_files: \n",
    "    place_result.append(place(file))\n",
    "  \n",
    "#del df['PLACE']\n",
    "df['place'] = pd.Series(place_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (place(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 31 private verbs \n",
    "#1\n",
    "def priv1(text_type):\n",
    "    def raw(text_type): \n",
    "        return text_type.count(('三思', 'verb'))+text_type.count(('三省', 'verb'))+text_type.count(('主张', 'verb'))+text_type.count(('了解', 'verb'))+text_type.count(('亲信', 'verb'))+text_type.count(('以为', 'verb'))+text_type.count(('企图', 'verb'))+text_type.count(('会意', 'verb'))+text_type.count(('伤心', 'verb'))+text_type.count(('估', 'verb'))+text_type.count(('估摸', 'verb'))+text_type.count(('估算', 'verb'))+text_type.count(('估计', 'verb'))+text_type.count(('估量', 'verb'))+text_type.count(('低估', 'verb'))+text_type.count(('体会', 'verb'))+text_type.count(('体味', 'verb'))+text_type.count(('信', 'verb'))+text_type.count(('信任', 'verb'))+text_type.count(('信赖', 'verb'))+text_type.count(('修省', 'verb'))+text_type.count(('假定', 'verb'))+text_type.count(('假想', 'verb'))+text_type.count(('允许', 'verb'))+text_type.count(('关心', 'verb'))+text_type.count(('关怀', 'verb'))+text_type.count(('内省', 'verb'))+text_type.count(('决定', 'verb'))+text_type.count(('决心', 'verb'))+text_type.count(('决意', 'verb'))+text_type.count(('决断', 'verb'))+text_type.count(('决计', 'verb'))+text_type.count(('准备', 'verb'))+text_type.count(('准许', 'verb'))+text_type.count(('凝思', 'verb'))+text_type.count(('凝想', 'verb'))+text_type.count(('凭信', 'verb'))+text_type.count(('分晓', 'verb'))+text_type.count(('切记', 'verb'))+text_type.count(('划算', 'verb'))+text_type.count(('判断', 'verb'))+text_type.count(('原谅', 'verb'))+text_type.count(('参悟', 'verb'))+text_type.count(('反对', 'verb'))+text_type.count(('反思', 'verb'))\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_1 = []\n",
    "for file in tagged_files:\n",
    "    p_1.append(priv1(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "def priv2(text_type):\n",
    "    def raw(text_type): \n",
    "        return text_type.count(('反省', 'verb'))+text_type.count(('发现', 'verb'))+text_type.count(('发觉', 'verb'))+text_type.count(('吃准', 'verb'))+text_type.count(('合计', 'verb'))+text_type.count(('合谋', 'verb'))+text_type.count(('同情', 'verb'))+text_type.count(('同意', 'verb'))+text_type.count(('否认', 'verb'))+text_type.count(('听信', 'verb'))+text_type.count(('听到', 'verb'))+text_type.count(('听见', 'verb'))+text_type.count(('哭', 'verb'))+text_type.count(('喜欢', 'verb'))+text_type.count(('喜爱', 'verb'))+text_type.count(('回味', 'verb'))+text_type.count(('回忆', 'verb'))+text_type.count(('回念', 'verb'))+text_type.count(('回想', 'verb'))+text_type.count(('回溯', 'verb'))+text_type.count(('回顾', 'verb'))+text_type.count(('图谋', 'verb'))+text_type.count(('图', 'verb'))+text_type.count(('坚信', 'verb'))+text_type.count(('多疑', 'verb'))+text_type.count(('失望', 'verb'))+text_type.count(('失身', 'verb'))+text_type.count(('妄图', 'verb'))+text_type.count(('妄断', 'verb'))+text_type.count(('宠信', 'verb'))+text_type.count(('害怕', 'verb'))+text_type.count(('察觉', 'verb'))+text_type.count(('寻思', 'verb'))+text_type.count(('尊敬', 'verb'))+text_type.count(('尊重', 'verb'))+text_type.count(('小心', 'verb'))+text_type.count(('希望', 'verb'))+text_type.count(('平静', 'verb'))+text_type.count(('幻想', 'verb'))+text_type.count(('当做', 'verb'))+text_type.count(('彻悟', 'verb'))+text_type.count(('得知', 'verb'))+text_type.count(('忆', 'verb'))+text_type.count(('忖度', 'verb'))+text_type.count(('忖量', 'verb'))+text_type.count(('忘', 'verb'))+text_type.count(('忘却', 'verb'))+text_type.count(('忘怀', 'verb'))\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_2 = []\n",
    "for file in tagged_files:\n",
    "    p_2.append(priv2(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "def priv3(text_type):\n",
    "    def raw(text_type): \n",
    "        return text_type.count(('忘掉', 'verb'))+text_type.count(('忘记', 'verb'))+text_type.count(('快乐', 'verb'))+text_type.count(('念', 'verb'))+text_type.count(('忽略', 'verb'))+text_type.count(('忽视', 'verb'))+text_type.count(('怀念', 'verb'))+text_type.count(('怀想', 'verb'))+text_type.count(('怀疑', 'verb'))+text_type.count(('怕', 'verb'))+text_type.count(('思忖', 'verb'))+text_type.count(('思想', 'verb'))+text_type.count(('思索', 'verb'))+text_type.count(('思维', 'verb'))+text_type.count(('思考', 'verb'))+text_type.count(('思虑', 'verb'))+text_type.count(('思量', 'verb'))+text_type.count(('恨', 'verb'))+text_type.count(('悟', 'verb'))+text_type.count(('悬想', 'verb'))+text_type.count(('情知', 'verb'))+text_type.count(('惊恐', 'verb'))+text_type.count(('想', 'verb'))+text_type.count(('想像', 'verb'))+text_type.count(('想来', 'verb'))+text_type.count(('想见', 'verb'))+text_type.count(('想象', 'verb'))+text_type.count(('愉快', 'verb'))+text_type.count(('意会', 'verb'))+text_type.count(('意想', 'verb'))+text_type.count(('意料', 'verb'))+text_type.count(('意识', 'verb'))+text_type.count(('感到', 'verb'))+text_type.count(('感动', 'verb'))+text_type.count(('感受', 'verb'))+text_type.count(('感悟', 'verb'))+text_type.count(('感想', 'verb'))+text_type.count(('感激', 'verb'))+text_type.count(('感觉', 'verb'))+text_type.count(('感觉', 'verb'))+text_type.count(('感谢', 'verb'))+text_type.count(('愤怒', 'verb'))\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_3 = []\n",
    "for file in tagged_files:\n",
    "    p_3.append(priv3(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "def priv4(text_type):\n",
    "    def raw(text_type): \n",
    "        return text_type.count(('愿意', 'verb'))+text_type.count(('懂', 'verb'))+text_type.count(('懂得', 'verb'))+text_type.count(('打算', 'verb'))+text_type.count(('承想', 'verb'))+text_type.count(('承认', 'verb'))+text_type.count(('担心', 'verb'))+text_type.count(('拥护', 'verb'))+text_type.count(('捉摸', 'verb'))+text_type.count(('掂掇', 'verb'))+text_type.count(('掂量', 'verb'))+text_type.count(('掌握', 'verb'))+text_type.count(('推度', 'verb'))+text_type.count(('推想', 'verb'))+text_type.count(('推敲', 'verb'))+text_type.count(('推断', 'verb'))+text_type.count(('推测', 'verb'))+text_type.count(('推理', 'verb'))+text_type.count(('推算', 'verb'))+text_type.count(('推见', 'verb'))+text_type.count(('措意', 'verb'))+text_type.count(('揆度', 'verb'))+text_type.count(('揣度', 'verb'))+text_type.count(('揣想', 'verb'))+text_type.count(('揣摩', 'verb'))+text_type.count(('揣摸', 'verb'))+text_type.count(('揣测', 'verb'))+text_type.count(('支持', 'verb'))+text_type.count(('放心', 'verb'))+text_type.count(('料想', 'verb'))+text_type.count(('料', 'verb'))+text_type.count(('斟酌', 'verb'))+text_type.count(('断定', 'verb'))+text_type.count(('明了', 'verb'))+text_type.count(('明察', 'verb'))+text_type.count(('明晓', 'verb'))+text_type.count(('明白', 'verb'))+text_type.count(('明知', 'verb'))+text_type.count(('明确', 'verb'))+text_type.count(('晓得', 'verb'))+text_type.count(('权衡', 'verb'))+text_type.count(('梦想', 'verb'))+text_type.count(('欢迎', 'verb'))+text_type.count(('欣赏', 'verb'))+text_type.count(('武断', 'verb'))+text_type.count(('死记', 'verb'))+text_type.count(('沉思', 'verb'))+text_type.count(('注意', 'verb'))+text_type.count(('洞察', 'verb'))+text_type.count(('洞彻', 'verb'))+text_type.count(('洞悉', 'verb'))+text_type.count(('洞晓', 'verb'))+text_type.count(('洞达', 'verb'))+text_type.count(('测度', 'verb'))+text_type.count(('浮想', 'verb'))+text_type.count(('淡忘', 'verb'))+text_type.count(('深信', 'verb'))+text_type.count(('深思', 'verb'))+text_type.count(('深省', 'verb'))+text_type.count(('深醒', 'verb'))+text_type.count(('清楚', 'verb'))+text_type.count(('清楚', 'verb'))+text_type.count(('满意', 'verb'))+text_type.count(('满足', 'verb'))+text_type.count(('激动', 'verb'))+text_type.count(('热爱', 'verb'))+text_type.count(('熟悉', 'verb'))+text_type.count(('熟知', 'verb'))+text_type.count(('熟虑', 'verb'))+text_type.count(('爱', 'verb'))+text_type.count(('爱好', 'verb'))+text_type.count(('牢记', 'verb'))+text_type.count(('犯疑', 'verb'))+text_type.count(('狂想', 'verb'))+text_type.count(('狐疑', 'verb'))+text_type.count(('猛醒', 'verb'))+text_type.count(('猜', 'verb'))+text_type.count(('猜度', 'verb'))+text_type.count(('猜忌', 'verb'))+text_type.count(('猜想', 'verb'))+text_type.count(('猜测', 'verb'))\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_4 = []\n",
    "for file in tagged_files:\n",
    "    p_4.append(priv4(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 \n",
    "def priv5(text_type):\n",
    "    def raw(text_type): \n",
    "        return text_type.count(('猜疑', 'verb'))+text_type.count(('玄想', 'verb'))+text_type.count(('理会', 'verb'))+text_type.count(('理解', 'verb'))+text_type.count(('琢磨', 'verb'))+text_type.count(('生气', 'verb'))+text_type.count(('生疑', 'verb'))+text_type.count(('畅想', 'verb'))+text_type.count(('留心', 'verb'))+text_type.count(('留神', 'verb'))+text_type.count(('疏忽', 'verb'))+text_type.count(('疑', 'verb'))+text_type.count(('疑心', 'verb'))+text_type.count(('疑猜', 'verb'))+text_type.count(('疑虑', 'verb'))+text_type.count(('疼', 'verb'))+text_type.count(('盘算', 'verb'))+text_type.count(('相信', 'verb'))+text_type.count(('盼望', 'verb'))+text_type.count(('省察', 'verb'))+text_type.count(('省悟', 'verb'))+text_type.count(('看', 'verb'))+text_type.count(('看到', 'verb'))+text_type.count(('看见', 'verb'))+text_type.count(('看透', 'verb'))+text_type.count(('着想', 'verb'))+text_type.count(('知', 'verb'))+text_type.count(('知悉', 'verb'))+text_type.count(('知晓', 'verb'))+text_type.count(('知道', 'verb'))+text_type.count(('确信', 'verb'))+text_type.count(('确定', 'verb'))+text_type.count(('确认', 'verb'))+text_type.count(('空想', 'verb'))+text_type.count(('立意', 'verb'))+text_type.count(('笃信', 'verb'))+text_type.count(('笑', 'verb'))+text_type.count(('答应', 'verb'))+text_type.count(('策划', 'verb'))+text_type.count(('筹划', 'verb'))+text_type.count(('筹算', 'verb'))+text_type.count(('筹谋', 'verb'))+text_type.count(('算', 'verb'))+text_type.count(('算计', 'verb'))+text_type.count(('粗估', 'verb'))+text_type.count(('约摸', 'verb'))+text_type.count(('置疑', 'verb'))+text_type.count(('考虑', 'verb'))+text_type.count(('考量', 'verb'))+text_type.count(('联想', 'verb'))+text_type.count(('腹诽', 'verb'))+text_type.count(('臆度', 'verb'))+text_type.count(('臆想', 'verb'))+text_type.count(('臆断', 'verb'))+text_type.count(('臆测', 'verb'))+text_type.count(('自信', 'verb'))+text_type.count(('自省', 'verb'))+text_type.count(('蒙', 'verb'))+text_type.count(('蓄念', 'verb'))+text_type.count(('蓄谋', 'verb'))+text_type.count(('衡量', 'verb'))+text_type.count(('裁度', 'verb'))+text_type.count(('要求', 'verb'))+text_type.count(('观察', 'verb'))+text_type.count(('觉察', 'verb'))+text_type.count(('觉得', 'verb'))+text_type.count(('觉悟', 'verb'))+text_type.count(('觉醒', 'verb'))+text_type.count(('警惕', 'verb'))+text_type.count(('警觉', 'verb'))+text_type.count(('计划', 'verb'))+text_type.count(('计算', 'verb'))+text_type.count(('计较', 'verb'))+text_type.count(('认为', 'verb'))+text_type.count(('认可', 'verb'))+text_type.count(('认同', 'verb'))+text_type.count(('认定', 'verb'))+text_type.count(('认得', 'verb'))+text_type.count(('认知', 'verb'))+text_type.count(('认识', 'verb'))\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_5 = []\n",
    "for file in tagged_files:\n",
    "    p_5.append(priv5(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "def priv6(text_type):\n",
    "    def raw(text_type): \n",
    "        return text_type.count(('讨厌', 'verb'))+text_type.count(('记', 'verb'))+text_type.count(('记取', 'verb'))+text_type.count(('记得', 'verb'))+text_type.count(('记忆', 'verb'))+text_type.count(('设想', 'verb'))+text_type.count(('识', 'verb'))+text_type.count(('试图', 'verb'))+text_type.count(('试想', 'verb'))+text_type.count(('详悉', 'verb'))+text_type.count(('误会', 'verb'))+text_type.count(('误解', 'verb'))+text_type.count(('谋划', 'verb'))+text_type.count(('谋算', 'verb'))+text_type.count(('谋虑', 'verb'))+text_type.count(('赞同', 'verb'))+text_type.count(('赞成', 'verb'))+text_type.count(('走神儿', 'verb'))+text_type.count(('起疑', 'verb'))+text_type.count(('轻信', 'verb'))+text_type.count(('轻视', 'verb'))+text_type.count(('迷信', 'verb'))+text_type.count(('迷信', 'verb'))+text_type.count(('追忆', 'verb'))+text_type.count(('追怀', 'verb'))+text_type.count(('追思', 'verb'))+text_type.count(('追想', 'verb'))+text_type.count(('通彻', 'verb'))+text_type.count(('通晓', 'verb'))+text_type.count(('通', 'verb'))+text_type.count(('遐想', 'verb'))+text_type.count(('遗忘', 'verb'))+text_type.count(('遥想', 'verb'))+text_type.count(('酌情', 'verb'))+text_type.count(('酌量', 'verb'))+text_type.count(('醒', 'verb'))+text_type.count(('醒悟', 'verb'))+text_type.count(('重视', 'verb'))+text_type.count(('铭记', 'verb'))+text_type.count(('阴谋', 'verb')) +text_type.count(('顾全', 'verb'))+text_type.count(('顾及', 'verb'))+text_type.count(('预卜', 'verb'))+text_type.count(('预想', 'verb'))+text_type.count(('预感', 'verb'))+text_type.count(('预料', 'verb'))+text_type.count(('预期', 'verb'))+text_type.count(('预测', 'verb'))+text_type.count(('预知', 'verb'))+text_type.count(('预见', 'verb'))+text_type.count(('预计', 'verb'))+text_type.count(('预谋', 'verb'))+text_type.count(('领会', 'verb'))+text_type.count(('领悟', 'verb'))+text_type.count(('领略', 'verb'))+text_type.count(('高估', 'verb'))+text_type.count(('高兴', 'verb'))+text_type.count(('默认', 'verb'))\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_6 = []\n",
    "for file in tagged_files:\n",
    "    p_6.append(priv6(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "random1=[sum(i) for i in zip(p_1, p_2)]\n",
    "random2=[sum(i) for i in zip(p_3, p_4)]\n",
    "random3=[sum(i) for i in zip(p_5, p_6)]\n",
    "random4=[sum(i) for i in zip(random1, random2)]\n",
    "final=[sum(i) for i in zip(random3, random4)]\n",
    "#for i in final: \n",
    "   # print (i)\n",
    "    \n",
    "df['PRIV'] = pd.Series(final)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 32 public verb PUBV\n",
    "#表示、称、道、说、讲、质疑、认为、坦言、指出、告诉、呼吁、解释、问、建议\n",
    "def pubv(text_type):\n",
    "    def raw(text_type): \n",
    "        return text_type.count(('表示', 'verb'))+text_type.count(('称', 'verb'))+text_type.count(('道', 'verb'))+text_type.count(('说', 'verb'))+text_type.count(('讲', 'verb'))+text_type.count(('质疑', 'verb'))+text_type.count(('认为', 'verb'))+text_type.count(('坦言', 'verb'))+text_type.count(('指出', 'verb'))+text_type.count(('告诉', 'verb'))+text_type.count(('呼吁', 'verb'))+text_type.count(('解释', 'verb'))+text_type.count(('问', 'verb'))+text_type.count(('建议', 'verb'))\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubv_result=[]\n",
    "for file in tagged_files: \n",
    "    pubv_result.append(pubv(file))\n",
    "    \n",
    "df['PUBV'] = pd.Series(pubv_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (pubv(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 33 RB adverbs 副词\n",
    "def rb(text_type):\n",
    "    def raw(text_type): \n",
    "        return str(text_type).count('adverb')\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb_result=[]\n",
    "for file in tagged_files: \n",
    "    rb_result.append(rb(file))\n",
    "    \n",
    "df['RB'] = pd.Series(rb_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (rb(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 35 negation 不、别、没\n",
    "#('别', 'adverb')、('不', 'adverb')\n",
    "#('没', 'verb')、('没', 'adverb')\n",
    "def mono_negation(text_type):\n",
    "    def raw(text_type): \n",
    "        return text_type.count(('别', 'adverb'))+text_type.count(('不', 'adverb'))+text_type.count(('没', 'verb'))+text_type.count(('没', 'adverb'))\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytic_negation_result=[]\n",
    "for file in tagged_files: \n",
    "    analytic_negation_result.append(analytic_negation(file))\n",
    "    \n",
    "df['analytic_negation'] = pd.Series(analytic_negation_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 36 disyllabic negation 没有\n",
    "#('没有', 'adverb')、('没有', 'verb')\n",
    "def di_negation(text_type):\n",
    "    def raw(text_type): \n",
    "        return text_type.count(('没有', 'adverb'))+text_type.count(('没有', 'verb'))\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthectic_negation_result=[]\n",
    "for file in tagged_files: \n",
    "    synthectic_negation_result.append(synthectic_negation(file))\n",
    "\n",
    "df['synthectic_negation'] = pd.Series(synthectic_negation_result)\n",
    "#del df['negation']\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (negation(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 37 TIME time word \n",
    "def time(text_type):\n",
    "    def raw(text_type): \n",
    "        return str(text_type).count('time word')\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_result=[]\n",
    "for file in tagged_files: \n",
    "    time_result.append(time(file))\n",
    "    \n",
    "df['TIME'] = pd.Series(time_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (time(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 40 WH 无定代词\n",
    "#('怎么', 'predicate interrogative pronoun')、('怎么样', 'predicate interrogative pronoun')\n",
    "#('怎样', 'predicate interrogative pronoun')\n",
    "#('为什么', 'predicate interrogative pronoun')\n",
    "#('如何', 'predicate interrogative pronoun')\n",
    "#('什么样', 'predicate interrogative pronoun')\n",
    "#search for predicate interrogative pronoun\n",
    "def wh(text_type):\n",
    "    def raw(text_type):\n",
    "        return str(text_type).count('predicate interrogative pronoun')\n",
    "    def normalized(text_type):                                                                                     \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_result=[]\n",
    "for file in tagged_files: \n",
    "    wh_result.append(wh(file))\n",
    "    \n",
    "df['WH'] = pd.Series(wh_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (wh(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 们 suffix\n",
    "def men(text_type):\n",
    "    def raw(text_type):\n",
    "        return text_type.count(('们', 'suffix'))\n",
    "    def normalized(text_type):                                                                                     \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_result=[]\n",
    "for file in tagged_files: \n",
    "    men_result.append(men(file))\n",
    "    \n",
    "df['men_suffix'] = pd.Series(men_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 41 monosyllabic verbs\n",
    "#1. convert corpora to dict # no None in values \n",
    "#2. select verbs from dict values \n",
    "#3. convert corresponding keys to list \n",
    "#4. return len(corresponding keys) == 2 or 1 / len (filtered dict)\n",
    "\n",
    "##dict would remove duplicates, not ideal\n",
    "d_list=[]\n",
    "for file in tagged_files: \n",
    "    d=dict(file)\n",
    "    d_list.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(tagged_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def mono_verbs(text): \n",
    "    verbs= {k:v for k,v in text.items() if re.match('.*verb', v)}\n",
    "    return round ((len([word for word in list(verbs.keys()) if len(word) == 1]) / len(text))*1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_verbs_result=[]\n",
    "for d in d_list: \n",
    "    mono_verbs_result.append(mono_verbs(d))\n",
    "    \n",
    "df['mono_verbs'] = pd.Series(mono_verbs_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for d in d_list: \n",
    "    #print (mono_verbs(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 42 disyllabic verbs\n",
    "def di_verbs(text): \n",
    "    verbs= {k:v for k,v in text.items() if re.match('.*verb', v)}\n",
    "    return round((len([word for word in list(verbs.keys()) if len(word) == 2]) / len(text))*1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "di_verbs_result=[]\n",
    "for d in d_list: \n",
    "    di_verbs_result.append(di_verbs(d))\n",
    "    \n",
    "df['di_verbs'] = pd.Series(di_verbs_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for d in d_list: \n",
    "    #print (di_verbs(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 41 classical function words 文言文功能词\n",
    "#('所', 'particle 所')、('将', 'adverb')、('将', 'preposition')\n",
    "#('之', 'particle 之')、('于', 'preposition')、('以', 'preposition')\n",
    "def classical_func(text_type):\n",
    "    def raw(text_type): \n",
    "        return text_type.count(('所', 'particle 所'))+text_type.count(('将', 'adverb'))+text_type.count(('将', 'preposition'))+text_type.count(('之', 'particle 之'))+text_type.count(('于', 'preposition'))+text_type.count(('以', 'preposition'))\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "classical_func_result=[]\n",
    "for file in tagged_files: \n",
    "    classical_func_result.append(classical_func(file))\n",
    "    \n",
    "df['classical_functional_words'] = pd.Series(classical_func_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (classical_func(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 50 lexical density \n",
    "#(noun+verb+adjective+numeral) / total\n",
    "def lexical_density(text): \n",
    "    verbs= {k:v for k,v in text.items() if re.match('.*verb', v)}\n",
    "    nouns= {k:v for k,v in text.items() if re.match('.*noun', v)}\n",
    "    adjectives= {k:v for k,v in text.items() if re.match('.*adjective', v)}\n",
    "    numerals= {k:v for k,v in text.items() if re.match('.*numeral', v)}\n",
    "    adverbs={k:v for k,v in text.items() if re.match('.*adverb', v)}\n",
    "    pronouns={k:v for k,v in text.items() if re.match('.*pronoun', v)}\n",
    "    return round(((len(verbs)+len(nouns)+len(adjectives)+len(numerals)-len(adverbs)-len(pronouns)) / len(text))*1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_density_result=[]\n",
    "for d in d_list: \n",
    "    lexical_density_result.append(lexical_density(d))\n",
    "    \n",
    "df['lexical_density'] = pd.Series(lexical_density_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for d in d_list: \n",
    "    #print (lexical_density(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 44 performative verbs, called ``performantive verb'' in ICTCLAS\n",
    "#进行、加以、予以\n",
    "def performative_verb(text_type):\n",
    "    def raw(text_type):\n",
    "        return str(text_type).count('performative verb')\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "performative_verb_result=[]\n",
    "for file in tagged_files: \n",
    "    performative_verb_result.append(performative_verb(file))\n",
    "    \n",
    "df['performative_verb'] = pd.Series(performative_verb_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (performative_verb(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 59 intransitive verbs\n",
    "def vi(text_type):\n",
    "    def raw(text_type):\n",
    "        return str(text_type).count('intransitive verb')\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_result=[]\n",
    "for file in tagged_files: \n",
    "    vi_result.append(vi(file))\n",
    "\n",
    "df['intransitive'] = pd.Series(vi_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (vi(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 60 auxiliary verbs\n",
    "#def aux_verb(text_type):\n",
    "    #def raw(text_type):\n",
    "        #return str(text_type).count('auxiliary verb')\n",
    "    #def normalized(text_type): \n",
    "        #return raw(text_type) / len(text_type)\n",
    "    #return round(normalized (text_type) * 1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aux_verb_result=[]\n",
    "#for file in tagged_files: \n",
    "    #aux_verb_result.append(aux_verb(file))\n",
    "\n",
    "#df['aux_verb'] = pd.Series(aux_verb_result)\n",
    "#df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (aux_verb(file))\n",
    "#del df['aux_verb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 61 auxiliary adjectives\n",
    "def aux_adj(text_type):\n",
    "    def raw(text_type):\n",
    "        return str(text_type).count('auxiliary adjective')\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_adj_result=[]\n",
    "for file in tagged_files: \n",
    "    aux_adj_result.append(aux_adj(file))\n",
    "\n",
    "df['aux_adj'] = pd.Series(aux_adj_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "   # print (aux_adj(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 45 attributive adjectives \n",
    "#ICTCLAS distinguishing word\n",
    "def aa(text_type):\n",
    "    def raw(text_type):\n",
    "        return str(text_type).count('distinguishing word')\n",
    "    def normalized(text_type):                                                                                     \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_result=[]\n",
    "for file in tagged_files: \n",
    "    aa_result.append(aa(file))\n",
    "    \n",
    "df['attributive_adjective'] = pd.Series(aa_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (aa(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 48 classifier 量词\n",
    "def classifier(text_type):\n",
    "    def raw(text_type): \n",
    "        return str(text_type).count('classifier')\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_result=[]\n",
    "for file in tagged_files: \n",
    "    classifier_result.append(classifier(file))\n",
    "    \n",
    "df['classifier'] = pd.Series(classifier_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (classifier(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 64 modal particles and interjections \n",
    "def particle(text_type):\n",
    "    def raw(text_type): \n",
    "        return str(text_type).count('modal particle')+str(text_type).count('interjection')\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_result=[]\n",
    "for file in tagged_files: \n",
    "    particle_result.append(particle(file))\n",
    "\n",
    "#df = pd.read_csv(\"/users/nannanliu/Research/MD/stats/individual/OC.csv\", header=0)\n",
    "#del df['particles']\n",
    "df['particle'] = pd.Series(particle_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (particle(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 49 modifying markers 的、地、得\n",
    "def modify_marker_di(text_type):\n",
    "    def raw(text_type):\n",
    "        return text_type.count(('地', 'particle 地'))\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "modify_marker_di_result=[]\n",
    "for file in tagged_files: \n",
    "    modify_marker_di_result.append(modify_marker_di(file))\n",
    "\n",
    "#del df['modifying_markers']\n",
    "df['modify_marker_di'] = pd.Series(modify_marker_di_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (modify_marker_di(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 50 modifying marker '的', 'particle 的/底'\n",
    "def modify_marker_dedi(text_type):\n",
    "    def raw(text_type):\n",
    "        return text_type.count(('的', 'particle 的/底'))\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "modify_marker_dedi_result=[]\n",
    "for file in tagged_files: \n",
    "    modify_marker_dedi_result.append(modify_marker_dedi(file))\n",
    "\n",
    "df['modify_marker_dedi'] = pd.Series(modify_marker_dedi_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (modify_marker_dedi(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 51 modifying marker '得', 'particle 得'\n",
    "def modify_marker_de(text_type):\n",
    "    def raw(text_type):\n",
    "        return text_type.count(('得', 'particle 得'))\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "modify_marker_de_result=[]\n",
    "for file in tagged_files: \n",
    "    modify_marker_de_result.append(modify_marker_de(file))\n",
    "\n",
    "df['modify_marker_de'] = pd.Series(modify_marker_de_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (modify_marker_de(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 53 disposal markers 把\n",
    "def disposal_marker_ba(text_type):\n",
    "    def raw(text_type):\n",
    "        return text_type.count(('把', 'preposition 把'))\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "disposal_marker_ba_result=[]\n",
    "for file in tagged_files: \n",
    "    disposal_marker_ba_result.append(disposal_marker_ba(file))\n",
    "    \n",
    "df['disposal_marker_ba'] = pd.Series(disposal_marker_ba_result)\n",
    "#del df['disposal_marker']\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (disposal_marker_ba(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 53 disposal markers 将\n",
    "#'将', 'adverb' is will/would\n",
    "def disposal_marker_jiang(text_type):\n",
    "    def raw(text_type):\n",
    "        return text_type.count(('将', 'preposition'))\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "disposal_marker_jiang_result=[]\n",
    "for file in tagged_files: \n",
    "    disposal_marker_jiang_result.append(disposal_marker_jiang(file))\n",
    "    \n",
    "df['disposal_marker_jiang'] = pd.Series(disposal_marker_jiang_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (disposal_marker_jiang(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 51 PEAS perfect aspect \n",
    "#('了', 'particle 了/喽')、('过', 'particle 过')\n",
    "def peas(text_type):\n",
    "    def raw(text_type):\n",
    "        return text_type.count(('了', 'particle 了/喽'))+text_type.count(('过', 'particle 过'))\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "peas_result=[]\n",
    "for file in tagged_files: \n",
    "    peas_result.append(peas(file))\n",
    "    \n",
    "df['PEAS'] = pd.Series(peas_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (peas(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 52 imperfect aspect \n",
    "#('着', 'particle 着')\n",
    "#('在', 'preposition')、('正在', 'adverb')\n",
    "#('起来', 'directional verb')、('下去', 'directional verb')\n",
    "def imperfect(text_type):\n",
    "    def raw(text_type):\n",
    "        return text_type.count(('着', 'particle 着'))+text_type.count(('在', 'preposition'))\\\n",
    "    +text_type.count(('正在', 'adverb'))+text_type.count(('起来', 'directional verb'))+text_type.count(('下去', 'directional verb'))\n",
    "    def normalized(text_type):                                                                                \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "imperfect_result=[]\n",
    "for file in tagged_files: \n",
    "    imperfect_result.append(imperfect(file))\n",
    "    \n",
    "df['imperfect_aspect'] = pd.Series(imperfect_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (imperfect(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 72 experiential_marker ('过', 'particle 过')\n",
    "def experiential(text_type):\n",
    "    def raw(text_type):\n",
    "        return text_type.count(('过', 'particle 过'))\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiential_result=[]\n",
    "for file in tagged_files: \n",
    "    experiential_result.append(experiential(file))\n",
    "    \n",
    "df['experiential'] = pd.Series(experiential_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (experiential(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 53\n",
    "#onomatopoeia\n",
    "def ono(text_type):\n",
    "    def raw(text_type):\n",
    "        return str(text_type).count('onomatopoeia')\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "ono_result=[]\n",
    "for file in tagged_files: \n",
    "    ono_result.append(ono(file))\n",
    "    \n",
    "df['onomatopoeia'] = pd.Series(ono_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "   # print (ono(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 54 descriptive words \n",
    "#ICTCLAS status word\n",
    "def descriptive(text_type):\n",
    "    def raw(text_type):\n",
    "        return str(text_type).count('status word')\n",
    "    def normalized(text_type):                                                                                     \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptive_result=[]\n",
    "for file in tagged_files: \n",
    "    descriptive_result.append(descriptive(file))\n",
    "    \n",
    "df['descriptive'] = pd.Series(descriptive_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (descriptive(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 55 similie\n",
    "def similie(text_type):\n",
    "    def raw(text_type):\n",
    "        return text_type.count(('仿佛', 'adverb'))+text_type.count(('宛若', 'verb'))\\\n",
    "    +text_type.count(('如', 'verb'))+str(text_type).count(('particle 一样/一般/似的/般'))\\\n",
    "    +text_type.count(('像', 'verb'))+text_type.count(('像', 'preposition'))\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "similie_result=[]\n",
    "for file in tagged_files: \n",
    "    similie_result.append(similie(file))\n",
    "    \n",
    "df['similie'] = pd.Series(similie_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (similie(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 78 idioms\n",
    "#def phrase(text_type):\n",
    "   # def raw(text_type):\n",
    "        #return str(text_type).count('phrase')\n",
    "    #def normalized(text_type): \n",
    "        #return raw(text_type) / len(text_type)\n",
    "    #return round(normalized (text_type) * 1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#phrase_result=[]\n",
    "#for file in tagged_files: \n",
    "    #phrase_result.append(phrase(file))\n",
    "\n",
    "#del df['idioms']\n",
    "#df['phrase'] = pd.Series(phrase_result)\n",
    "#df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (phrase(file))\n",
    "#del df['phrase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 85 numerals\n",
    "#def numeral(text_type):\n",
    "    #def raw(text_type):\n",
    "        #return str(text_type).count('numeral')\n",
    "    #def normalized(text_type): \n",
    "       # return raw(text_type) / len(text_type)\n",
    "    #return round(normalized (text_type) * 1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numeral_result=[]\n",
    "#for file in tagged_files: \n",
    "    #numeral_result.append(numeral(file))\n",
    "    \n",
    "#df['numeral'] = pd.Series(numeral_result)\n",
    "#df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (numeral(file))\n",
    "#del df['numeral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 86 question mark\n",
    "def question(text_type):\n",
    "    def raw(text_type):\n",
    "        return text_type.count(('？', 'question mark'))\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_result=[]\n",
    "for file in tagged_files: \n",
    "    question_result.append(question(file))\n",
    "    \n",
    "df['question'] = pd.Series(question_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "   # print (question(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 87 exclamation mark\n",
    "def exclamation(text_type):\n",
    "    def raw(text_type):\n",
    "        return str(text_type).count('exclamation mark')\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclamation_result=[]\n",
    "for file in tagged_files: \n",
    "    exclamation_result.append(exclamation(file))\n",
    "    \n",
    "df['exclamation'] = pd.Series(exclamation_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (exclamation(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 88 quotation mark\n",
    "def quote(text_type):\n",
    "    def raw(text_type):\n",
    "        return str(text_type).count('quotation mark') / 2\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote_result=[]\n",
    "for file in tagged_files: \n",
    "    quote_result.append(quote(file))\n",
    "    \n",
    "df['quote'] = pd.Series(quote_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (quote(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 89 dash\n",
    "def dash(text_type):\n",
    "    def raw(text_type):\n",
    "        return str(text_type).count('dash') \n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "dash_result=[]\n",
    "for file in tagged_files: \n",
    "    dash_result.append(dash(file))\n",
    "    \n",
    "df['dash'] = pd.Series(dash_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (dash(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 90 book parenthesis\n",
    "def book(text_type):\n",
    "    def raw(text_type):\n",
    "        return text_type.count(('《', 'left parenthesis/bracket')) \n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_result=[]\n",
    "for file in tagged_files: \n",
    "    book_result.append(book(file))\n",
    "    \n",
    "df['book'] = pd.Series(book_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (book(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 92 transcribed names, including toponym and personal name\n",
    "#def transcribed(text_type):\n",
    "    #def raw(text_type):\n",
    "        #return str(text_type).count('transcribed') \n",
    "    #def normalized(text_type): \n",
    "        #return raw(text_type) / len(text_type)\n",
    "    #return round(normalized (text_type) * 1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transcribed_result=[]\n",
    "#for file in tagged_files: \n",
    "    #transcribed_result.append(transcribed(file))\n",
    "    \n",
    "#df['transcribed'] = pd.Series(transcribed_result)\n",
    "#df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (transcribed(file))\n",
    "#del df['transcribed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 93 person names, personal name - transcribed personal names\n",
    "def person(text_type):\n",
    "    def raw(text_type):\n",
    "        return str(text_type).count('personal name')+str(text_type).count('Chinese')- str(text_type).count('transcribed personal name')\n",
    "    def normalized(text_type): \n",
    "        return raw(text_type) / len(text_type)\n",
    "    return round(normalized (text_type) * 1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_result=[]\n",
    "for file in tagged_files: \n",
    "    person_result.append(person(file))\n",
    "    \n",
    "df['person'] = pd.Series(person_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (person(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 46 noun + noun combination\n",
    "#need to plus one for the final result because \n",
    "#\"if atag pattern matches at overlapping locations, the leftmost match takes precedence\"\n",
    "#Bird et al.\n",
    "import nltk\n",
    "import re \n",
    "grammar_noun=r\"NN: {<noun.*><noun.*>}\"\n",
    "cp_noun=nltk.RegexpParser(grammar_noun) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn(text): \n",
    "    def parse(text): \n",
    "        return cp_noun.parse(text)\n",
    "    nn_list=[]\n",
    "    for subtree in parse(text).subtrees():\n",
    "        if subtree.label() == 'NN': \n",
    "            nn_list.append(subtree.leaves())\n",
    "    return round ((len(nn_list)/len(text))*1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_result=[]\n",
    "for file in tagged_files: \n",
    "    nn_result.append(nn(file))\n",
    "#del df['nounnoun']   \n",
    "df['nn'] = pd.Series(nn_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (nn(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#consecutive verbs\n",
    "grammar_verb=r\"VV: {<verb.*><verb.*>}\"\n",
    "cp_verb=nltk.RegexpParser(grammar_verb)\n",
    "#feature 56 verb + verb combination\n",
    "def vv(text): \n",
    "    def parse(text): \n",
    "        return cp_verb.parse(text)\n",
    "    vv_list=[]\n",
    "    for subtree in parse(text).subtrees():\n",
    "        if subtree.label() == 'VV': \n",
    "            vv_list.append(subtree.leaves())\n",
    "    return round ((len(vv_list)/len(text))*1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv_result=[]\n",
    "for file in tagged_files: \n",
    "    vv_result.append(vv(file))\n",
    "    \n",
    "df['verbverb'] = pd.Series(vv_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tagged_files: \n",
    "    #print (vv(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del df['Unnamed: 0.1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('/users/nannanliu/Research/MD/stats/individual/OC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd  \n",
    "#df = pd.read_csv(\"/users/nannanliu/Research/MD/stats/individual/OC.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop(df.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
