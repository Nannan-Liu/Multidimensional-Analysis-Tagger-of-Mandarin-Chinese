{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['女士们', '，', '先生们', '，', '上午好', '。', ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk \n",
    "from nltk.corpus import CategorizedPlaintextCorpusReader\n",
    "corpus = CategorizedPlaintextCorpusReader(\n",
    "    'your/text/path/',\n",
    "    r'(?!\\.).*\\.txt',\n",
    "    cat_pattern=os.path.join(r'(neg|pos)', '.*',),\n",
    "    encoding='utf-8')\n",
    "corpus.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort text files by their names, if necessary\n",
    "import fileinput\n",
    "import fnmatch\n",
    "\n",
    "files=corpus.fileids()\n",
    "sample_files=[]\n",
    "\n",
    "for f in files: \n",
    "    if fnmatch.fnmatch(f, 'sample.txt'):\n",
    "        print (f)\n",
    "        sample_files.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynlpir\n",
    "pynlpir.open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora=[]\n",
    "for file in OC_files: \n",
    "    sub_corpora=corpus.raw(file)\n",
    "    corpora.append(sub_corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notice that here pos_names is set to `parent' because we need to add one sub_feature\n",
    "tagged_files=[]\n",
    "for sub_corpora in corpora: \n",
    "    tagged_file=pynlpir.segment(sub_corpora, pos_tagging=True, pos_names='parent')\n",
    "    tagged_files.append(tagged_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_list=[]\n",
    "for file in tagged_files: \n",
    "    none_list.append([s for s in file if None in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "print (none_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(10): \n",
    "    for n, i in enumerate(tagged_files[j]):\n",
    "        if i == ('\\r新华社', None):\n",
    "            tagged_files[j][n] = ('\\r新华社', 'noun-proper')\n",
    "        if i == ('新华社', None):\n",
    "            tagged_files[j][n] = ('新华社', 'noun-proper')\n",
    "        if i == ('\\r新华网', None):\n",
    "            tagged_files[j][n] = ('\\r新华网', 'noun-proper')\n",
    "        if i == ('新华网', None):\n",
    "            tagged_files[j][n] = ('新华网', 'noun-proper')\n",
    "        if i == ('中新网', None):\n",
    "            tagged_files[j][n] = ('中新网', 'noun-proper')\n",
    "        if i == ('人民网', None):\n",
    "            tagged_files[j][n] = ('人民网', 'noun-proper')\n",
    "        if i == ('\\r中国青年网', None):\n",
    "            tagged_files[j][n] = ('\\r中国青年网', 'noun-proper')\n",
    "        if i == ('中评社', None):\n",
    "            tagged_files[j][n] = ('中评社', 'noun-proper')\n",
    "        if i == ('\\r中国日报网', None):\n",
    "            tagged_files[j][n] = ('\\r中国日报网', 'noun-proper')\n",
    "        if i == ('南华早报', None):\n",
    "            tagged_files[j][n] = ('南华早报', 'noun-proper')\n",
    "        if i == ('\\r国际在线', None):\n",
    "            tagged_files[j][n] = ('\\r国际在线', 'noun-proper')\n",
    "        if i == ('新华社', None):\n",
    "            tagged_files[j][n] = ('新华社', 'noun-proper')\n",
    "        if i == ('派', None): \n",
    "            tagged_files[j][n] = ('派', 'noun-verb')\n",
    "        if i == ('网民', None): \n",
    "            tagged_files[j][n] = ('网民', 'noun')\n",
    "        if i == ('屌丝', None):\n",
    "            tagged_files[j][n] = ('屌丝', 'noun')\n",
    "        if i == ('\\r屌丝', None):\n",
    "            tagged_files[j][n] = ('\\r屌丝', 'noun')\n",
    "        if i == ('富帅', None):\n",
    "            tagged_files[j][n] = ('富帅', 'noun')\n",
    "        if i == ('解构', None): \n",
    "            tagged_files[j][n] = ('解构', 'noun-verb')\n",
    "        if i == ('身份卑微', None): \n",
    "            tagged_files[j][n] = ('身份卑微', 'adjective')\n",
    "        if i == ('\\r南方日报', None): \n",
    "            tagged_files[j][n] = ('\\r南方日报', 'noun')\n",
    "        if i == ('法新社', None):\n",
    "            tagged_files[j][n] = ('法新社', 'noun-proper')\n",
    "        if i == ('美联社', None):\n",
    "            tagged_files[j][n] = ('美联社', 'noun-proper')\n",
    "        if i == ('路透社', None):\n",
    "            tagged_files[j][n] = ('路透社', 'noun-proper')\n",
    "        if i == ('环球时报', None):\n",
    "            tagged_files[j][n] = ('环球时报', 'noun-proper')\n",
    "        if i == ('飞机', None):\n",
    "            tagged_files[j][n] = ('飞机', 'noun')\n",
    "        if i == ('甲', None): \n",
    "            tagged_files[j][n] = ('甲', 'numeral')\n",
    "        if i == ('乙', None): \n",
    "            tagged_files[j][n] = ('乙', 'numeral')\n",
    "        if i == ('丙', None): \n",
    "            tagged_files[j][n] = ('丙', 'numeral')\n",
    "        if i == ('丁', None): \n",
    "            tagged_files[j][n] = ('丁', 'numeral')\n",
    "        if i == ('辰', None): \n",
    "            tagged_files[j][n] = ('辰', 'numeral')\n",
    "        if i == ('癸', None): \n",
    "            tagged_files[j][n] = ('癸', 'numeral')  \n",
    "        if i == ('戊', None): \n",
    "            tagged_files[j][n] = ('戊', 'numeral')\n",
    "        if i == ('巳', None): \n",
    "            tagged_files[j][n] = ('巳', 'numeral')\n",
    "        if i == ('\\u3000', None): \n",
    "            tagged_files[j][n] = ('\\u3000', 'None')\n",
    "        if i == ('贴吧', None): \n",
    "            tagged_files[j][n] = ('贴吧', 'noun')  \n",
    "        if i == (' ', None): \n",
    "            tagged_files[j][n] = (' ', 'empty')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def verb_de(text): \n",
    "    verb_list=list(map(list, zip([item for item in text if re.match(r'\\bverb\\b', item[1])], text[1:])))\n",
    "    flat_list = [item for sublist in verb_list for item in sublist]\n",
    "    return round ((flat_list.count(('的', 'particle'))/len(text))*1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "df = pd.read_csv(\"text_path/linguistic_features.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_de_result=[]\n",
    "for file in tagged_files: \n",
    "    verb_de_result.append(verb_de(file))\n",
    "\n",
    "df['verb_de'] = pd.Series(verb_de_result)\n",
    "df.to_csv('text_path/linguistic_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NOMZ'] = df['NOMZ'] + df['verb_de']\n",
    "del df['verb_de']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('text_path/linguistic_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
